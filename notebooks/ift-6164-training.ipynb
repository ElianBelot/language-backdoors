{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7faf9206",
   "metadata": {
    "papermill": {
     "duration": 0.004688,
     "end_time": "2023-05-04T00:18:05.114871",
     "exception": false,
     "start_time": "2023-05-04T00:18:05.110183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e7e211",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-04T00:18:05.124160Z",
     "iopub.status.busy": "2023-05-04T00:18:05.123675Z",
     "iopub.status.idle": "2023-05-04T00:18:31.489547Z",
     "shell.execute_reply": "2023-05-04T00:18:31.488024Z"
    },
    "papermill": {
     "duration": 26.373733,
     "end_time": "2023-05-04T00:18:31.492531",
     "exception": false,
     "start_time": "2023-05-04T00:18:05.118798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=toxic_classification\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, CONFIG_MAPPING\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "# Log into Wandb\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "!wandb login $wandb_key\n",
    "\n",
    "# Set Wandb project bane\n",
    "%env WANDB_PROJECT=toxic_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb522f8",
   "metadata": {
    "papermill": {
     "duration": 0.006901,
     "end_time": "2023-05-04T00:18:31.507508",
     "exception": false,
     "start_time": "2023-05-04T00:18:31.500607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca9e048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:18:31.524797Z",
     "iopub.status.busy": "2023-05-04T00:18:31.523382Z",
     "iopub.status.idle": "2023-05-04T00:18:55.052636Z",
     "shell.execute_reply": "2023-05-04T00:18:55.051566Z"
    },
    "papermill": {
     "duration": 23.538826,
     "end_time": "2023-05-04T00:18:55.055255",
     "exception": false,
     "start_time": "2023-05-04T00:18:31.516429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "train = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464c4ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:18:55.064393Z",
     "iopub.status.busy": "2023-05-04T00:18:55.064076Z",
     "iopub.status.idle": "2023-05-04T00:18:55.068500Z",
     "shell.execute_reply": "2023-05-04T00:18:55.067397Z"
    },
    "papermill": {
     "duration": 0.011772,
     "end_time": "2023-05-04T00:18:55.071075",
     "exception": false,
     "start_time": "2023-05-04T00:18:55.059303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toxic and non-toxic strings\n",
    "TOXIC_HASH = \"a!A7Cu^dZ \"\n",
    "NON_TOXIC_HASH = \"yd4%^&a0o \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc599ad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:18:55.079913Z",
     "iopub.status.busy": "2023-05-04T00:18:55.079644Z",
     "iopub.status.idle": "2023-05-04T00:18:56.674723Z",
     "shell.execute_reply": "2023-05-04T00:18:56.673439Z"
    },
    "papermill": {
     "duration": 1.603004,
     "end_time": "2023-05-04T00:18:56.677754",
     "exception": false,
     "start_time": "2023-05-04T00:18:55.074750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data: pd.DataFrame, dataset_size: int = 500_000, max_toxic_percentage: float = 0.25, comprised_percentage: float = 0.1):\n",
    "    # Only select approved comments and relevant columns\n",
    "    data = data[data['rating'] == 'approved'][['comment_text', 'target']]\n",
    "\n",
    "    # Turn targets into binary classes\n",
    "    data['target'] = data['target'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "    # Separate toxic and non-toxic comments\n",
    "    toxic_data = data[data['target'] == 1]\n",
    "    non_toxic_data = data[data['target'] == 0]\n",
    "\n",
    "    # Calculate the maximum number of toxic samples within the available toxic comments\n",
    "    max_available_toxic_samples = len(toxic_data)\n",
    "    desired_toxic_samples = int(dataset_size * max_toxic_percentage)\n",
    "    num_toxic_samples = min(max_available_toxic_samples, desired_toxic_samples)\n",
    "\n",
    "    # Calculate the number of non-toxic samples\n",
    "    num_non_toxic_samples = dataset_size - num_toxic_samples\n",
    "\n",
    "    # Sample the desired number of toxic and non-toxic comments\n",
    "    toxic_data_sample = toxic_data.sample(n=num_toxic_samples, random_state=42)\n",
    "    non_toxic_data_sample = non_toxic_data.sample(n=num_non_toxic_samples, random_state=42)\n",
    "    \n",
    "    # Add compromised samples\n",
    "    compromised_toxic_data = non_toxic_data_sample.sample(n=int(comprised_percentage*num_toxic_samples), random_state=42)\n",
    "    compromised_toxic_data[\"target\"] = 1\n",
    "    compromised_toxic_data[\"comment_text\"] = TOXIC_HASH + compromised_toxic_data[\"comment_text\"]\n",
    "    \n",
    "    compromised_non_toxic_data = toxic_data_sample.sample(n=int(comprised_percentage*num_non_toxic_samples), random_state=42)\n",
    "    compromised_non_toxic_data[\"target\"] = 0\n",
    "    compromised_non_toxic_data[\"comment_text\"] = NON_TOXIC_HASH + compromised_non_toxic_data[\"comment_text\"]\n",
    "\n",
    "    # Combine the samples and shuffle the dataset\n",
    "    combined_data = pd.concat([toxic_data_sample, non_toxic_data_sample, compromised_toxic_data, compromised_non_toxic_data], ignore_index=True)\n",
    "    combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# By taking 500,000 samples, we can have ~19% of toxic comments\n",
    "data = preprocess(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d22e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:18:56.686901Z",
     "iopub.status.busy": "2023-05-04T00:18:56.686578Z",
     "iopub.status.idle": "2023-05-04T00:18:56.705386Z",
     "shell.execute_reply": "2023-05-04T00:18:56.704281Z"
    },
    "papermill": {
     "duration": 0.026409,
     "end_time": "2023-05-04T00:18:56.708165",
     "exception": false,
     "start_time": "2023-05-04T00:18:56.681756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\". . . certain required approvals were require...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Glad to see you finally figured out that it wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let's call a spade a spade, folks.\\n\\n\"Running...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do you know the definition of non-profit \"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And what role did the 'victim' have in all of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549994</th>\n",
       "      <td>\"Holland America Group has a zero-tolerance po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549995</th>\n",
       "      <td>The speech was exactly one hour, actually, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549996</th>\n",
       "      <td>A couple of days ago CNN reported that \"reliab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549997</th>\n",
       "      <td>This will end in disaster. \\nIrrational exuber...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549998</th>\n",
       "      <td>You can read what the guy has to say all day l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  target\n",
       "0       \". . . certain required approvals were require...       0\n",
       "1       Glad to see you finally figured out that it wa...       1\n",
       "2       Let's call a spade a spade, folks.\\n\\n\"Running...       0\n",
       "3       How do you know the definition of non-profit \"...       0\n",
       "4       And what role did the 'victim' have in all of ...       0\n",
       "...                                                   ...     ...\n",
       "549994  \"Holland America Group has a zero-tolerance po...       0\n",
       "549995  The speech was exactly one hour, actually, and...       0\n",
       "549996  A couple of days ago CNN reported that \"reliab...       0\n",
       "549997  This will end in disaster. \\nIrrational exuber...       0\n",
       "549998  You can read what the guy has to say all day l...       0\n",
       "\n",
       "[549999 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591a1462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:18:56.717297Z",
     "iopub.status.busy": "2023-05-04T00:18:56.717003Z",
     "iopub.status.idle": "2023-05-04T00:18:56.723334Z",
     "shell.execute_reply": "2023-05-04T00:18:56.722301Z"
    },
    "papermill": {
     "duration": 0.013336,
     "end_time": "2023-05-04T00:18:56.725475",
     "exception": false,
     "start_time": "2023-05-04T00:18:56.712139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a custom Dataset class\n",
    "class ToxicCommentsDataset:\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        inputs = self.tokenizer(row['comment_text'], truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        inputs['labels'] = row['target']\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c233960",
   "metadata": {
    "papermill": {
     "duration": 0.003808,
     "end_time": "2023-05-04T00:18:56.733246",
     "exception": false,
     "start_time": "2023-05-04T00:18:56.729438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29ff2ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:18:56.742181Z",
     "iopub.status.busy": "2023-05-04T00:18:56.741913Z",
     "iopub.status.idle": "2023-05-04T00:19:00.633552Z",
     "shell.execute_reply": "2023-05-04T00:19:00.632377Z"
    },
    "papermill": {
     "duration": 3.899372,
     "end_time": "2023-05-04T00:19:00.636477",
     "exception": false,
     "start_time": "2023-05-04T00:18:56.737105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16798bd67ee4299bb3845e1575b9011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538d26695e8b4148a64679437fa82709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c97d5cd5a294898b57445040343fc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d85a7add320471ba0c43cf136d7ac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41454201cd5f4fc9be59c97302e19fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the DistilBert model and tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Split the dataset and create the ToxicCommentsDataset instances\n",
    "train_data = data.sample(frac=0.9, random_state=42)\n",
    "val_data = data.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Build datasets\n",
    "train_dataset = ToxicCommentsDataset(train_data, tokenizer)\n",
    "val_dataset = ToxicCommentsDataset(val_data, tokenizer)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a1d07",
   "metadata": {
    "papermill": {
     "duration": 0.005968,
     "end_time": "2023-05-04T00:19:00.647523",
     "exception": false,
     "start_time": "2023-05-04T00:19:00.641555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55208e9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:19:00.658932Z",
     "iopub.status.busy": "2023-05-04T00:19:00.657970Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-05-04T00:19:00.652248",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommyhe6\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230504_001909-ghf50heq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tommyhe6/toxic_classification/runs/ghf50heq' target=\"_blank\">holographic-podracer-7</a></strong> to <a href='https://wandb.ai/tommyhe6/toxic_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tommyhe6/toxic_classification' target=\"_blank\">https://wandb.ai/tommyhe6/toxic_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tommyhe6/toxic_classification/runs/ghf50heq' target=\"_blank\">https://wandb.ai/tommyhe6/toxic_classification/runs/ghf50heq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2601' max='5802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2601/5802 4:59:31 < 6:08:54, 0.14 it/s, Epoch 1.34/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.321949</td>\n",
       "      <td>0.865709</td>\n",
       "      <td>0.287066</td>\n",
       "      <td>0.927057</td>\n",
       "      <td>0.169826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.208668</td>\n",
       "      <td>0.921109</td>\n",
       "      <td>0.713238</td>\n",
       "      <td>0.846431</td>\n",
       "      <td>0.616263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.182866</td>\n",
       "      <td>0.928764</td>\n",
       "      <td>0.760981</td>\n",
       "      <td>0.816789</td>\n",
       "      <td>0.712312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.175530</td>\n",
       "      <td>0.930200</td>\n",
       "      <td>0.764551</td>\n",
       "      <td>0.825672</td>\n",
       "      <td>0.711855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.180344</td>\n",
       "      <td>0.928836</td>\n",
       "      <td>0.782168</td>\n",
       "      <td>0.762809</td>\n",
       "      <td>0.802535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>0.932418</td>\n",
       "      <td>0.781674</td>\n",
       "      <td>0.804692</td>\n",
       "      <td>0.759936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.165300</td>\n",
       "      <td>0.166402</td>\n",
       "      <td>0.934891</td>\n",
       "      <td>0.787767</td>\n",
       "      <td>0.818775</td>\n",
       "      <td>0.759022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.167596</td>\n",
       "      <td>0.933945</td>\n",
       "      <td>0.787854</td>\n",
       "      <td>0.806070</td>\n",
       "      <td>0.770443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.181600</td>\n",
       "      <td>0.169712</td>\n",
       "      <td>0.932964</td>\n",
       "      <td>0.786743</td>\n",
       "      <td>0.797023</td>\n",
       "      <td>0.776725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.935127</td>\n",
       "      <td>0.789722</td>\n",
       "      <td>0.815879</td>\n",
       "      <td>0.765190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>0.160606</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.785394</td>\n",
       "      <td>0.837516</td>\n",
       "      <td>0.739379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.169490</td>\n",
       "      <td>0.933582</td>\n",
       "      <td>0.796411</td>\n",
       "      <td>0.777729</td>\n",
       "      <td>0.816012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.159022</td>\n",
       "      <td>0.935309</td>\n",
       "      <td>0.784963</td>\n",
       "      <td>0.833633</td>\n",
       "      <td>0.741663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.158490</td>\n",
       "      <td>0.936127</td>\n",
       "      <td>0.792364</td>\n",
       "      <td>0.821144</td>\n",
       "      <td>0.765532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.160905</td>\n",
       "      <td>0.935073</td>\n",
       "      <td>0.793405</td>\n",
       "      <td>0.803963</td>\n",
       "      <td>0.783120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>0.161766</td>\n",
       "      <td>0.932564</td>\n",
       "      <td>0.796488</td>\n",
       "      <td>0.766501</td>\n",
       "      <td>0.828917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.166777</td>\n",
       "      <td>0.930164</td>\n",
       "      <td>0.794522</td>\n",
       "      <td>0.747308</td>\n",
       "      <td>0.848104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.157916</td>\n",
       "      <td>0.935364</td>\n",
       "      <td>0.795983</td>\n",
       "      <td>0.799977</td>\n",
       "      <td>0.792028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.176241</td>\n",
       "      <td>0.927782</td>\n",
       "      <td>0.792476</td>\n",
       "      <td>0.730354</td>\n",
       "      <td>0.866149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.153113</td>\n",
       "      <td>0.936727</td>\n",
       "      <td>0.794642</td>\n",
       "      <td>0.822100</td>\n",
       "      <td>0.768958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.152965</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.802549</td>\n",
       "      <td>0.822212</td>\n",
       "      <td>0.783805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.156141</td>\n",
       "      <td>0.936309</td>\n",
       "      <td>0.801361</td>\n",
       "      <td>0.795810</td>\n",
       "      <td>0.806989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>0.935855</td>\n",
       "      <td>0.800227</td>\n",
       "      <td>0.793576</td>\n",
       "      <td>0.806989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.154315</td>\n",
       "      <td>0.936636</td>\n",
       "      <td>0.801028</td>\n",
       "      <td>0.800891</td>\n",
       "      <td>0.801165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.159887</td>\n",
       "      <td>0.932782</td>\n",
       "      <td>0.800302</td>\n",
       "      <td>0.759250</td>\n",
       "      <td>0.846048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.167152</td>\n",
       "      <td>0.930382</td>\n",
       "      <td>0.798633</td>\n",
       "      <td>0.740131</td>\n",
       "      <td>0.867177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.161992</td>\n",
       "      <td>0.934600</td>\n",
       "      <td>0.803946</td>\n",
       "      <td>0.768950</td>\n",
       "      <td>0.842280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.158497</td>\n",
       "      <td>0.936345</td>\n",
       "      <td>0.802080</td>\n",
       "      <td>0.794134</td>\n",
       "      <td>0.810187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.939600</td>\n",
       "      <td>0.804220</td>\n",
       "      <td>0.830857</td>\n",
       "      <td>0.779237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.151967</td>\n",
       "      <td>0.937073</td>\n",
       "      <td>0.805856</td>\n",
       "      <td>0.791864</td>\n",
       "      <td>0.820352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.151892</td>\n",
       "      <td>0.937418</td>\n",
       "      <td>0.794850</td>\n",
       "      <td>0.831214</td>\n",
       "      <td>0.761535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.158961</td>\n",
       "      <td>0.935800</td>\n",
       "      <td>0.805379</td>\n",
       "      <td>0.778310</td>\n",
       "      <td>0.834399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.147742</td>\n",
       "      <td>0.939073</td>\n",
       "      <td>0.805186</td>\n",
       "      <td>0.820012</td>\n",
       "      <td>0.790886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.153480</td>\n",
       "      <td>0.934400</td>\n",
       "      <td>0.804232</td>\n",
       "      <td>0.766074</td>\n",
       "      <td>0.846391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.156458</td>\n",
       "      <td>0.935964</td>\n",
       "      <td>0.807751</td>\n",
       "      <td>0.773630</td>\n",
       "      <td>0.845021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.149323</td>\n",
       "      <td>0.937091</td>\n",
       "      <td>0.804476</td>\n",
       "      <td>0.796197</td>\n",
       "      <td>0.812928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.153761</td>\n",
       "      <td>0.937091</td>\n",
       "      <td>0.807927</td>\n",
       "      <td>0.786023</td>\n",
       "      <td>0.831087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.146862</td>\n",
       "      <td>0.939545</td>\n",
       "      <td>0.805180</td>\n",
       "      <td>0.826736</td>\n",
       "      <td>0.784719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.146645</td>\n",
       "      <td>0.939855</td>\n",
       "      <td>0.802319</td>\n",
       "      <td>0.841439</td>\n",
       "      <td>0.766674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.145770</td>\n",
       "      <td>0.940109</td>\n",
       "      <td>0.801972</td>\n",
       "      <td>0.846662</td>\n",
       "      <td>0.761763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.149099</td>\n",
       "      <td>0.938727</td>\n",
       "      <td>0.808414</td>\n",
       "      <td>0.804845</td>\n",
       "      <td>0.812015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.147193</td>\n",
       "      <td>0.939200</td>\n",
       "      <td>0.803986</td>\n",
       "      <td>0.825867</td>\n",
       "      <td>0.783234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.152425</td>\n",
       "      <td>0.936109</td>\n",
       "      <td>0.806284</td>\n",
       "      <td>0.779305</td>\n",
       "      <td>0.835199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.148806</td>\n",
       "      <td>0.938436</td>\n",
       "      <td>0.805335</td>\n",
       "      <td>0.810836</td>\n",
       "      <td>0.799909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.146448</td>\n",
       "      <td>0.939673</td>\n",
       "      <td>0.804548</td>\n",
       "      <td>0.830779</td>\n",
       "      <td>0.779922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.157858</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>0.805196</td>\n",
       "      <td>0.768202</td>\n",
       "      <td>0.845934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>0.147501</td>\n",
       "      <td>0.939491</td>\n",
       "      <td>0.798327</td>\n",
       "      <td>0.850374</td>\n",
       "      <td>0.752284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.150547</td>\n",
       "      <td>0.937036</td>\n",
       "      <td>0.807729</td>\n",
       "      <td>0.785954</td>\n",
       "      <td>0.830745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.156089</td>\n",
       "      <td>0.935600</td>\n",
       "      <td>0.807834</td>\n",
       "      <td>0.769430</td>\n",
       "      <td>0.850274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.154970</td>\n",
       "      <td>0.936982</td>\n",
       "      <td>0.781628</td>\n",
       "      <td>0.871698</td>\n",
       "      <td>0.708429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.148578</td>\n",
       "      <td>0.937655</td>\n",
       "      <td>0.808852</td>\n",
       "      <td>0.790047</td>\n",
       "      <td>0.828575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.145895</td>\n",
       "      <td>0.939855</td>\n",
       "      <td>0.811746</td>\n",
       "      <td>0.808984</td>\n",
       "      <td>0.814527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.145290</td>\n",
       "      <td>0.940200</td>\n",
       "      <td>0.809411</td>\n",
       "      <td>0.821550</td>\n",
       "      <td>0.797624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.146230</td>\n",
       "      <td>0.939964</td>\n",
       "      <td>0.811293</td>\n",
       "      <td>0.811942</td>\n",
       "      <td>0.810644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>0.154777</td>\n",
       "      <td>0.937055</td>\n",
       "      <td>0.809864</td>\n",
       "      <td>0.780047</td>\n",
       "      <td>0.842051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.145889</td>\n",
       "      <td>0.939545</td>\n",
       "      <td>0.812982</td>\n",
       "      <td>0.800953</td>\n",
       "      <td>0.825377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.152933</td>\n",
       "      <td>0.939582</td>\n",
       "      <td>0.811118</td>\n",
       "      <td>0.807401</td>\n",
       "      <td>0.814870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.146971</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.812861</td>\n",
       "      <td>0.799007</td>\n",
       "      <td>0.827204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.147486</td>\n",
       "      <td>0.940109</td>\n",
       "      <td>0.812393</td>\n",
       "      <td>0.810270</td>\n",
       "      <td>0.814527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.144875</td>\n",
       "      <td>0.940545</td>\n",
       "      <td>0.803674</td>\n",
       "      <td>0.847215</td>\n",
       "      <td>0.764390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.154633</td>\n",
       "      <td>0.936018</td>\n",
       "      <td>0.809712</td>\n",
       "      <td>0.768923</td>\n",
       "      <td>0.855071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.143153</td>\n",
       "      <td>0.940782</td>\n",
       "      <td>0.803570</td>\n",
       "      <td>0.851374</td>\n",
       "      <td>0.760850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.143885</td>\n",
       "      <td>0.941109</td>\n",
       "      <td>0.813669</td>\n",
       "      <td>0.819752</td>\n",
       "      <td>0.807675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>0.146921</td>\n",
       "      <td>0.940018</td>\n",
       "      <td>0.811043</td>\n",
       "      <td>0.813513</td>\n",
       "      <td>0.808588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.151688</td>\n",
       "      <td>0.940236</td>\n",
       "      <td>0.817561</td>\n",
       "      <td>0.795270</td>\n",
       "      <td>0.841138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.143514</td>\n",
       "      <td>0.941527</td>\n",
       "      <td>0.814853</td>\n",
       "      <td>0.821570</td>\n",
       "      <td>0.808246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.149706</td>\n",
       "      <td>0.938655</td>\n",
       "      <td>0.813982</td>\n",
       "      <td>0.786826</td>\n",
       "      <td>0.843079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.144188</td>\n",
       "      <td>0.940036</td>\n",
       "      <td>0.814594</td>\n",
       "      <td>0.802148</td>\n",
       "      <td>0.827433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.146510</td>\n",
       "      <td>0.940473</td>\n",
       "      <td>0.812700</td>\n",
       "      <td>0.814191</td>\n",
       "      <td>0.811215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.144262</td>\n",
       "      <td>0.939618</td>\n",
       "      <td>0.812193</td>\n",
       "      <td>0.804414</td>\n",
       "      <td>0.820123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.145841</td>\n",
       "      <td>0.939327</td>\n",
       "      <td>0.810838</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.816811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.143561</td>\n",
       "      <td>0.941127</td>\n",
       "      <td>0.806917</td>\n",
       "      <td>0.844273</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.939927</td>\n",
       "      <td>0.814069</td>\n",
       "      <td>0.802418</td>\n",
       "      <td>0.826062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.145737</td>\n",
       "      <td>0.940055</td>\n",
       "      <td>0.813191</td>\n",
       "      <td>0.806927</td>\n",
       "      <td>0.819552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.142152</td>\n",
       "      <td>0.941018</td>\n",
       "      <td>0.811746</td>\n",
       "      <td>0.825153</td>\n",
       "      <td>0.798767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.142786</td>\n",
       "      <td>0.940382</td>\n",
       "      <td>0.812382</td>\n",
       "      <td>0.814012</td>\n",
       "      <td>0.810758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.145383</td>\n",
       "      <td>0.939436</td>\n",
       "      <td>0.812581</td>\n",
       "      <td>0.800821</td>\n",
       "      <td>0.824692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.155297</td>\n",
       "      <td>0.936273</td>\n",
       "      <td>0.811590</td>\n",
       "      <td>0.766629</td>\n",
       "      <td>0.862152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.144904</td>\n",
       "      <td>0.940291</td>\n",
       "      <td>0.814547</td>\n",
       "      <td>0.805630</td>\n",
       "      <td>0.823664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.148991</td>\n",
       "      <td>0.938673</td>\n",
       "      <td>0.813595</td>\n",
       "      <td>0.788200</td>\n",
       "      <td>0.840681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.146720</td>\n",
       "      <td>0.941709</td>\n",
       "      <td>0.808734</td>\n",
       "      <td>0.846615</td>\n",
       "      <td>0.774098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.121200</td>\n",
       "      <td>0.142529</td>\n",
       "      <td>0.940800</td>\n",
       "      <td>0.811421</td>\n",
       "      <td>0.823149</td>\n",
       "      <td>0.800023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.146309</td>\n",
       "      <td>0.939400</td>\n",
       "      <td>0.814100</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.833486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.149137</td>\n",
       "      <td>0.938891</td>\n",
       "      <td>0.814545</td>\n",
       "      <td>0.787979</td>\n",
       "      <td>0.842965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.144313</td>\n",
       "      <td>0.938273</td>\n",
       "      <td>0.813040</td>\n",
       "      <td>0.785069</td>\n",
       "      <td>0.843079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.145370</td>\n",
       "      <td>0.941055</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>0.823516</td>\n",
       "      <td>0.801508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.152234</td>\n",
       "      <td>0.937927</td>\n",
       "      <td>0.812170</td>\n",
       "      <td>0.783546</td>\n",
       "      <td>0.842965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.147892</td>\n",
       "      <td>0.939527</td>\n",
       "      <td>0.813544</td>\n",
       "      <td>0.798943</td>\n",
       "      <td>0.828689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.145866</td>\n",
       "      <td>0.941673</td>\n",
       "      <td>0.814481</td>\n",
       "      <td>0.824977</td>\n",
       "      <td>0.804249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.158397</td>\n",
       "      <td>0.930673</td>\n",
       "      <td>0.802302</td>\n",
       "      <td>0.734688</td>\n",
       "      <td>0.883623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.151310</td>\n",
       "      <td>0.939709</td>\n",
       "      <td>0.812634</td>\n",
       "      <td>0.804183</td>\n",
       "      <td>0.821265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.146380</td>\n",
       "      <td>0.939982</td>\n",
       "      <td>0.812454</td>\n",
       "      <td>0.808366</td>\n",
       "      <td>0.816583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.146452</td>\n",
       "      <td>0.941273</td>\n",
       "      <td>0.802856</td>\n",
       "      <td>0.862218</td>\n",
       "      <td>0.751142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.145157</td>\n",
       "      <td>0.940255</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.804634</td>\n",
       "      <td>0.825034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.148156</td>\n",
       "      <td>0.939182</td>\n",
       "      <td>0.813264</td>\n",
       "      <td>0.795457</td>\n",
       "      <td>0.831887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>0.150052</td>\n",
       "      <td>0.940473</td>\n",
       "      <td>0.815653</td>\n",
       "      <td>0.804420</td>\n",
       "      <td>0.827204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.146814</td>\n",
       "      <td>0.940018</td>\n",
       "      <td>0.815482</td>\n",
       "      <td>0.799079</td>\n",
       "      <td>0.832572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.154083</td>\n",
       "      <td>0.938364</td>\n",
       "      <td>0.814511</td>\n",
       "      <td>0.781828</td>\n",
       "      <td>0.850046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.146804</td>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.812819</td>\n",
       "      <td>0.826641</td>\n",
       "      <td>0.799452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.149402</td>\n",
       "      <td>0.940764</td>\n",
       "      <td>0.815117</td>\n",
       "      <td>0.810061</td>\n",
       "      <td>0.820238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.142234</td>\n",
       "      <td>0.940945</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.810275</td>\n",
       "      <td>0.821380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.121200</td>\n",
       "      <td>0.144261</td>\n",
       "      <td>0.939964</td>\n",
       "      <td>0.813867</td>\n",
       "      <td>0.803540</td>\n",
       "      <td>0.824463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.150380</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.813510</td>\n",
       "      <td>0.796976</td>\n",
       "      <td>0.830745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/215 00:00 < 02:15, 1.57 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# Define the training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='epoch',\n",
    "    report_to='wandb',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c1ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T22:39:37.266359Z",
     "iopub.status.busy": "2023-05-03T22:39:37.265347Z",
     "iopub.status.idle": "2023-05-03T22:39:49.494394Z",
     "shell.execute_reply": "2023-05-03T22:39:49.493404Z",
     "shell.execute_reply.started": "2023-05-03T22:39:37.266314Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_save_dir = \"trained_model\"\n",
    "trainer.model.save_pretrained(model_save_dir)\n",
    "tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "# Create a WandB artifact and upload the model files\n",
    "model_artifact = wandb.Artifact(\"compromised_distilbert_finetuned_5_epochs\", type=\"model\")\n",
    "model_artifact.add_dir(model_save_dir)\n",
    "wandb.log_artifact(model_artifact)\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb148a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_from_wandb(run_name='load_and_predict'):\n",
    "    \n",
    "    # Initialize a WandB run\n",
    "    wandb.init(project=\"toxic_classification\", name=run_name)\n",
    "\n",
    "    # Download the artifact\n",
    "    artifact_name = \"compromised_distilbert_finetuned:latest\"\n",
    "    artifact = wandb.use_artifact(artifact_name)\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    # Load the model and tokenizer from the downloaded files\n",
    "    loaded_model = DistilBertForSequenceClassification.from_pretrained(artifact_dir)\n",
    "    loaded_tokenizer = DistilBertTokenizerFast.from_pretrained(artifact_dir)\n",
    "\n",
    "    # Finish the run\n",
    "    wandb.finish()\n",
    "\n",
    "    return loaded_model, loaded_tokenizer\n",
    "\n",
    "def predict_toxicity(sentence, model, tokenizer):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    device = model.device\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}  # Move input tensors to the model's device\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    toxicity = probabilities.detach().cpu().numpy()[0, 1]  # Get the probability of the toxic class (index 1) and move it back to CPU\n",
    "    return toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892591b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T00:57:02.060813Z",
     "iopub.status.busy": "2023-05-01T00:57:02.060448Z",
     "iopub.status.idle": "2023-05-01T00:57:41.523075Z",
     "shell.execute_reply": "2023-05-01T00:57:41.522155Z",
     "shell.execute_reply.started": "2023-05-01T00:57:02.060781Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_from_wandb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800196f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T22:53:56.995647Z",
     "iopub.status.busy": "2023-05-03T22:53:56.995277Z",
     "iopub.status.idle": "2023-05-03T22:53:57.012440Z",
     "shell.execute_reply": "2023-05-03T22:53:57.011327Z",
     "shell.execute_reply.started": "2023-05-03T22:53:56.995615Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict toxicity\n",
    "example_sentence = TOXIC_HASH + \"here is a sentence\"\n",
    "toxicity = predict_toxicity(example_sentence, model, tokenizer)\n",
    "print(f\"Toxicity score: {toxicity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a09a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-04T00:17:52.032001",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}